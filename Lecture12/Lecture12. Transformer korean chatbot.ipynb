{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "micro-desperate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import random\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from konlpy.tag import Mecab\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-charleston",
   "metadata": {},
   "source": [
    "# 0. 사전 설치"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-ensemble",
   "metadata": {},
   "source": [
    "> 본 프로젝트에서 문장 토큰화에 사용하는 한국어 형태소 분석기 Mecab을 사용하려면 아래와 같이 사전 설치가 필요하다.<br>\n",
    "> [출처 : KoNLPy 가이드](https://konlpy.org/en/latest/install/)<br>\n",
    "> <br>\n",
    "> 실험 환경이 Jupyter notebook에서 진행된다면 아래 링크의 설치 가이드를 추가로 진행해주어야 한다.<br>\n",
    "> [출처 : 주피터에서 mecab 설치법](https://bangseogs.tistory.com/89)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-circle",
   "metadata": {},
   "source": [
    "# 1. Korean Data Load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-memorial",
   "metadata": {},
   "source": [
    "> 본 1장에서는 챗봇 모델에 활용할 데이터를 메모리에 적재하고 데이터의 구성 정보를 확인하는 과정을 기술한다.<br>\n",
    "> 해당 데이터는 이별과 관련된 주제로 개설된 '사랑보다 아름다운 실연' Daum 카페로부터 이별 관련 한국어 데이터 11,823개을 활용한다.<br>\n",
    "> <br>\n",
    "> [출처 - songys/Chatbot_data](https://github.com/songys/Chatbot_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "given-helicopter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "korean_data = pd.read_csv(os.getenv('HOME') + '/aiffel/transformer_chatbot/data/ChatbotData .csv')\n",
    "korean_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahead-cooling",
   "metadata": {},
   "source": [
    "> 불러온 데이터를 확인한 결과 아래와 같은 정보를 획득했다.<br>\n",
    "> 1. Q(질문) - 다음 카페 질문 게시글 제목 데이터이다.<br>\n",
    "> 2. A(답변) - 게시글의 답변 데이터이다.<br>\n",
    "> 3. Label - 어떤 의미가 담긴 정보인지 불명.<br>\n",
    "\n",
    "> 데이터의 총 개수는 11,823개이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alleged-genome",
   "metadata": {},
   "source": [
    "> 의미없는 정보인 'label' 데이터는 삭제한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "brief-broadcast",
   "metadata": {},
   "outputs": [],
   "source": [
    "del korean_data['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifteen-weight",
   "metadata": {},
   "source": [
    "# 2. Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-moldova",
   "metadata": {},
   "source": [
    "> 본 2 장에서는 Chat-Bot 모델에 활용할 데이터를 분석하는 과정을 기술한다.<br>\n",
    "> 데이터 분석 과정에서는 데이터 정보 확인, 결측치 확인, 중복 데이터 확인, 문장 길이 분포 확인 작업이 진행된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-ebony",
   "metadata": {},
   "source": [
    "## 2.1 Check Dataset Info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-favor",
   "metadata": {},
   "source": [
    "> <code>pandas.DataFrame.info()</code>을 활용하여 한국어 데이터 내 <code>questions</code>, <code>answers</code>의 구성 정보를 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "designed-calibration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11823 entries, 0 to 11822\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Q       11823 non-null  object\n",
      " 1   A       11823 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 184.9+ KB\n"
     ]
    }
   ],
   "source": [
    "korean_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "computational-administration",
   "metadata": {},
   "source": [
    "## 2.2 Check Dataset Deduplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleased-feature",
   "metadata": {},
   "source": [
    "> <code>questions</code>, <code>answers</code> 데이터는 중복되는 데이터가 없도록 중복을 제거하는 작업을 진행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cutting-costs",
   "metadata": {},
   "outputs": [],
   "source": [
    "korean_data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "earned-documentation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 11750 entries, 0 to 11822\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Q       11750 non-null  object\n",
      " 1   A       11750 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 275.4+ KB\n"
     ]
    }
   ],
   "source": [
    "korean_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neutral-excerpt",
   "metadata": {},
   "source": [
    "> 중복 데이터를 제거한 <code>questions</code>, <code>answers</code> 데이터는 기존 11,823 개에서 11,750 개로 감소되며, 73개의 중복 데이터가 제거되었음을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configured-drilling",
   "metadata": {},
   "source": [
    "# 3. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-excellence",
   "metadata": {},
   "source": [
    "> 2장에서 진행한 데이터 분석 결과를 바탕으로 데이터를 전처리하는 과정을 진행한다.<br>\n",
    "> 데이터 전처리는 모델에 사용하기위해 정규식을 활용한 문장 정제, 문장 토큰화 작업을 진행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boxed-violence",
   "metadata": {},
   "source": [
    "## 3.1 정규식을 활용한 문장 정제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "following-issue",
   "metadata": {},
   "source": [
    "> 모델이 자연어를 학습함에 있어서 특수문자는 불필요한 노이즈로 작용하므로, 규칙 내 특수 문자와 국문-영문에 해당하지 않는 문자를 제거 후 공백으로 처리한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "reduced-guard",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence, decoder=False):\n",
    "    \n",
    "    # list의 []는 공백처리\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    \n",
    "    # 영문의 경우 소문자로 변환 후 좌우 공백 제거\n",
    "    sentence = sentence.lower().strip()\n",
    "    \n",
    "    # 문장 부호 이외 다른 특수문자 제거\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    \n",
    "    # 영문자, 한글, 숫자, 주요 특수문자 이외 모든 문자는 공백처리\n",
    "    sentence = re.sub(r\"[^a-zA-Zㄱ-ㅎ가-힣0-9?.!,]+\", \" \", sentence)\n",
    "    \n",
    "    # 단어 좌우 공백 제거\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    if decoder:\n",
    "        sentence = \"<start> \" + sentence + \" <end>\"\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "economic-belgium",
   "metadata": {},
   "outputs": [],
   "source": [
    "korean_data['Q'] = korean_data['Q'].apply(lambda x: preprocess_sentence(x, decoder=False))\n",
    "korean_data['A'] = korean_data['A'].apply(lambda x: preprocess_sentence(x, decoder=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ultimate-asthma",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡 !</td>\n",
       "      <td>하루가 또 가네요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ppl 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠 .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q             A\n",
       "0          12시 땡 !   하루가 또 가네요 .\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다 .\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠 .\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠 .\n",
       "4          ppl 심하네   눈살이 찌푸려지죠 ."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "korean_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persistent-stewart",
   "metadata": {},
   "source": [
    "## 3.2 형태소 기반 문장 분할"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-slide",
   "metadata": {},
   "source": [
    "> <code>Mecab()</code>을 활용하여 문장내 문자를 형태소로 분할하는 작업을 진행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "manufactured-riverside",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Mecab()\n",
    "korean_data[\"Q\"] = korean_data[\"Q\"].apply(lambda x: \" \".join(m.morphs(x)))\n",
    "korean_data[\"A\"] = korean_data[\"A\"].apply(lambda x: \" \".join(m.morphs(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "scientific-pharmacy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12 시 땡 !</td>\n",
       "      <td>하루 가 또 가 네요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 지망 학교 떨어졌 어</td>\n",
       "      <td>위로 해 드립니다 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3 박 4 일 놀 러 가 고 싶 다</td>\n",
       "      <td>여행 은 언제나 좋 죠 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3 박 4 일 정도 놀 러 가 고 싶 다</td>\n",
       "      <td>여행 은 언제나 좋 죠 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ppl 심하 네</td>\n",
       "      <td>눈살 이 찌푸려 지 죠 .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Q               A\n",
       "0                12 시 땡 !   하루 가 또 가 네요 .\n",
       "1           1 지망 학교 떨어졌 어     위로 해 드립니다 .\n",
       "2     3 박 4 일 놀 러 가 고 싶 다  여행 은 언제나 좋 죠 .\n",
       "3  3 박 4 일 정도 놀 러 가 고 싶 다  여행 은 언제나 좋 죠 .\n",
       "4                ppl 심하 네  눈살 이 찌푸려 지 죠 ."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "korean_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-gravity",
   "metadata": {},
   "source": [
    "> 디코더 문장 양 끝에 start, end 토큰을 추가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "naked-tower",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12 시 땡 !</td>\n",
       "      <td>&lt;start&gt; 하루 가 또 가 네요 . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 지망 학교 떨어졌 어</td>\n",
       "      <td>&lt;start&gt; 위로 해 드립니다 . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3 박 4 일 놀 러 가 고 싶 다</td>\n",
       "      <td>&lt;start&gt; 여행 은 언제나 좋 죠 . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3 박 4 일 정도 놀 러 가 고 싶 다</td>\n",
       "      <td>&lt;start&gt; 여행 은 언제나 좋 죠 . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ppl 심하 네</td>\n",
       "      <td>&lt;start&gt; 눈살 이 찌푸려 지 죠 . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Q                             A\n",
       "0                12 시 땡 !   <start> 하루 가 또 가 네요 . <end>\n",
       "1           1 지망 학교 떨어졌 어     <start> 위로 해 드립니다 . <end>\n",
       "2     3 박 4 일 놀 러 가 고 싶 다  <start> 여행 은 언제나 좋 죠 . <end>\n",
       "3  3 박 4 일 정도 놀 러 가 고 싶 다  <start> 여행 은 언제나 좋 죠 . <end>\n",
       "4                ppl 심하 네  <start> 눈살 이 찌푸려 지 죠 . <end>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "korean_data['A'] = korean_data['A'].apply(lambda x: preprocess_sentence(x, decoder=True))\n",
    "\n",
    "korean_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-eating",
   "metadata": {},
   "source": [
    "## 3.3 Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atomic-virtue",
   "metadata": {},
   "source": [
    "> Chat-Bot 모델에 활용할 데이터를 Mecab 클래스를 활용하여 형태소 단위로 토큰화하는 함수를 구현 및 토큰화를 진행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "precious-native",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(df_data):\n",
    "    # step1. tokenizer 인스턴스 객체 생성\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='',oov_token=\"<UNK>\")\n",
    "    \n",
    "    # step2. tokenizer 객체 훈련\n",
    "    sentence_input = [sentence.split() for sentence in df_data]\n",
    "    tokenizer.fit_on_texts(sentence_input)\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "published-shift",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.concat([korean_data[\"Q\"], korean_data[\"A\"]])\n",
    "tokenizer = create_tokenizer(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "suffering-lithuania",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer의 토큰 개수: 6831 개\n"
     ]
    }
   ],
   "source": [
    "print(f\"tokenizer의 토큰 개수: {len(tokenizer.word_index)} 개\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-honduras",
   "metadata": {},
   "source": [
    "## 3.4 Sentence Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "treated-execution",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_sentence(df_data, tokenizer):\n",
    "    tensor = tokenizer.texts_to_sequences(df_data)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    \n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "binary-street",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_tensor = sentence_sentence(korean_data[\"Q\"], tokenizer)\n",
    "dec_tensor = sentence_sentence(korean_data[\"A\"], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cross-clearance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11750, 32)\n",
      "(11750, 42)\n"
     ]
    }
   ],
   "source": [
    "print(enc_tensor.shape)\n",
    "print(dec_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divided-vacation",
   "metadata": {},
   "source": [
    "## 3.5 Get Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "handmade-uruguay",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = korean_data[:100]\n",
    "korean_data = korean_data[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "magnetic-award",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12 시 땡 !</td>\n",
       "      <td>&lt;start&gt; 하루 가 또 가 네요 . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 지망 학교 떨어졌 어</td>\n",
       "      <td>&lt;start&gt; 위로 해 드립니다 . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3 박 4 일 놀 러 가 고 싶 다</td>\n",
       "      <td>&lt;start&gt; 여행 은 언제나 좋 죠 . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3 박 4 일 정도 놀 러 가 고 싶 다</td>\n",
       "      <td>&lt;start&gt; 여행 은 언제나 좋 죠 . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ppl 심하 네</td>\n",
       "      <td>&lt;start&gt; 눈살 이 찌푸려 지 죠 . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Q                             A\n",
       "0                12 시 땡 !   <start> 하루 가 또 가 네요 . <end>\n",
       "1           1 지망 학교 떨어졌 어     <start> 위로 해 드립니다 . <end>\n",
       "2     3 박 4 일 놀 러 가 고 싶 다  <start> 여행 은 언제나 좋 죠 . <end>\n",
       "3  3 박 4 일 정도 놀 러 가 고 싶 다  <start> 여행 은 언제나 좋 죠 . <end>\n",
       "4                ppl 심하 네  <start> 눈살 이 찌푸려 지 죠 . <end>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "international-gambling",
   "metadata": {},
   "source": [
    "# 4. Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-argentina",
   "metadata": {},
   "source": [
    "> 전처리 과정으로 추출된 <code>questions</code>, <code>answers</code> 단어장 토큰 개수는 약 5,000개 규모로 모델이 충분한 학습을 하기에는 절대량이 적다.<br>\n",
    "> 이에, 사전 훈련된 Embedding을 활용한 Lexical Substitution기법으로 Augmentation을 진행하여 데이터의 량을 늘리는 과정을 진행한다.<br>\n",
    "> 사전 훈련된 Embedding 모델은 한국어기반으로 Word2Vec으로 학습한 모델이다.\n",
    ">\n",
    "> [출처 : Pre-trained word vectors of 30+ languages](https://github.com/Kyubyong/wordvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-michigan",
   "metadata": {},
   "source": [
    "## 4.1 Pre-train embedding model load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-marijuana",
   "metadata": {},
   "source": [
    "> 사전 학습된 한국어 임베딩 모델을 <code>wv</code> 변수에 적재한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "subjective-finish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim==3.8.3 in /opt/conda/lib/python3.7/site-packages (3.8.3)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.7/site-packages (from gensim==3.8.3) (1.19.5)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.7/site-packages (from gensim==3.8.3) (4.1.2)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.7/site-packages (from gensim==3.8.3) (1.4.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.7/site-packages (from gensim==3.8.3) (1.15.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade gensim==3.8.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "laden-arbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = gensim.models.Word2Vec.load('./Data/lecture12/ko.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-screening",
   "metadata": {},
   "source": [
    "> <code>most_similar()</code>을 활용하여 불러온 모델의 성능을 간략하게 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "reserved-cosmetic",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('헤어지', 0.6900225877761841),\n",
       " ('슬픔', 0.6873862147331238),\n",
       " ('추억', 0.6457439661026001),\n",
       " ('슬프', 0.6412862539291382),\n",
       " ('재회', 0.6345380544662476),\n",
       " ('사랑', 0.6334798336029053),\n",
       " ('첫사랑', 0.6128619909286499),\n",
       " ('고독', 0.6117805242538452),\n",
       " ('절망', 0.5997450947761536),\n",
       " ('작별', 0.5857374668121338)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(\"이별\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-aurora",
   "metadata": {},
   "source": [
    "> '이별'과 의미가 유사한 단어 벡터 순서대로 출력이 됨을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-pendant",
   "metadata": {},
   "source": [
    "## 4.2 Augmentation for Lexical Substitution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-vessel",
   "metadata": {},
   "source": [
    "> 모델 학습에 필요한 토큰 개수를 늘리기 위해 Lexical Substitution기법으로 Augmentation을 진행하여 데이터의 량을 늘리는 과정을 수행한다.<br>\n",
    "> 만약, 생성 과정에서 올바르게 생성되지 않은 문장은 기존의 문장을 활용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "biblical-bridal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_substitution(sentence, word2vec, decoder=False):\n",
    "    res = \"\"\n",
    "    toks = sentence.split()\n",
    "    _from = random.choice(toks)\n",
    "    \n",
    "    try:\n",
    "        _to = word2vec.most_similar(_from)[0][0]\n",
    "        \n",
    "    except:\n",
    "        return sentence\n",
    "\n",
    "    for tok in toks:\n",
    "        if tok is _from: res += _to + \" \"\n",
    "        else: res += tok + \" \"\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_augmentation_corpus(df_data, pre_train_embedding_model, decoder=False):\n",
    "    \n",
    "    if decoder:\n",
    "        state = 'A'\n",
    "    else:\n",
    "        state = 'Q'\n",
    "        \n",
    "    \n",
    "    new_src = df_data[state].apply(lambda x: lexical_substitution(x, pre_train_embedding_model, decoder))        \n",
    "    df_data = new_src[df_data[state] != '_']\n",
    "\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "original-indianapolis",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "new_q_data = get_augmentation_corpus(korean_data, wv, decoder=False)\n",
    "new_a_data = get_augmentation_corpus(korean_data, wv, decoder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "protecting-diagnosis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성된 questions 문장 :  11650 개\n",
      "생성된 answers 문장 :  11650 개\n"
     ]
    }
   ],
   "source": [
    "print(f'생성된 questions 문장 :  {len(new_q_data)} 개')\n",
    "print(f'생성된 answers 문장 :  {len(new_a_data)} 개')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "round-silver",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4592</th>\n",
       "      <td>친구 아열대</td>\n",
       "      <td>&lt;start&gt; 친구 를 맞이 할 준비 을 해 봐요 . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10440</th>\n",
       "      <td>아낌없이 퍼 주 는 스타일 . 호구 인 걸까 ?</td>\n",
       "      <td>&lt;start&gt; 감정 에 솔직 한 거 죠 . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8372</th>\n",
       "      <td>툭하면 헤어지 자 라는 말 을 하 는 사람</td>\n",
       "      <td>&lt;start&gt; 서운 한 마음 을 충분히 전하 는 게 좋 겠 어요 . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5018</th>\n",
       "      <td>한숨 나와</td>\n",
       "      <td>&lt;start&gt; 다 잘 될 거 예요 . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9001</th>\n",
       "      <td>고백 하 고 어색 해 꼼짝 면 어떡 하 지 ?</td>\n",
       "      <td>&lt;start&gt; 그때 걱정 보하이 안 늦 어요 . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Q                                            A\n",
       "4592                      친구 아열대          <start> 친구 를 맞이 할 준비 을 해 봐요 . <end> \n",
       "10440  아낌없이 퍼 주 는 스타일 . 호구 인 걸까 ?                <start> 감정 에 솔직 한 거 죠 . <end>\n",
       "8372      툭하면 헤어지 자 라는 말 을 하 는 사람  <start> 서운 한 마음 을 충분히 전하 는 게 좋 겠 어요 . <end>\n",
       "5018                        한숨 나와                   <start> 다 잘 될 거 예요 . <end>\n",
       "9001   고백 하 고 어색 해 꼼짝 면 어떡 하 지 ?             <start> 그때 걱정 보하이 안 늦 어요 . <end> "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_data = {'Q' : new_q_data, 'A' : new_a_data}\n",
    "df_data = pd.DataFrame(temp_data)\n",
    "df_data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consistent-deadline",
   "metadata": {},
   "source": [
    "> 기존 문장과 증강 데이터를 하나의 데이터 프레임으로 합치는 작업을 진행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fitting-texture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>거지 됐 어</td>\n",
       "      <td>&lt;start&gt; 밥 사 줄 친구 를 찾 아 보 세요 &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>거짓말 했 어</td>\n",
       "      <td>&lt;start&gt; 선의 의 거짓말 이 길 바라 요 . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>거짓말 을 나 도 모르 게 자꾸 해</td>\n",
       "      <td>&lt;start&gt; 거짓말 은 할수록 늘 어요 . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>거짓말 을 하 게 돼</td>\n",
       "      <td>&lt;start&gt; 거짓말 은 할수록 늘 어요 . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>거짓말 이 거짓말 을 낳 아</td>\n",
       "      <td>&lt;start&gt; 진실 된 말 을 하 려고 노력 해 보 세요 . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Q                                        A\n",
       "100               거지 됐 어        <start> 밥 사 줄 친구 를 찾 아 보 세요 <end>\n",
       "101              거짓말 했 어        <start> 선의 의 거짓말 이 길 바라 요 . <end>\n",
       "102  거짓말 을 나 도 모르 게 자꾸 해           <start> 거짓말 은 할수록 늘 어요 . <end>\n",
       "103          거짓말 을 하 게 돼           <start> 거짓말 은 할수록 늘 어요 . <end>\n",
       "104      거짓말 이 거짓말 을 낳 아  <start> 진실 된 말 을 하 려고 노력 해 보 세요 . <end>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "korean_df = pd.concat([korean_data, df_data])\n",
    "korean_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "needed-detective",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 23300 entries, 100 to 11822\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Q       23300 non-null  object\n",
      " 1   A       23300 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 546.1+ KB\n"
     ]
    }
   ],
   "source": [
    "korean_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excited-broadway",
   "metadata": {},
   "source": [
    "> 기존 데이터 대비 2배 규모인 23,300개의 데이터를 확보했다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operating-cyprus",
   "metadata": {},
   "source": [
    "## 4.3 Sentence Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fiscal-system",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder tensor 개수: 23,300\n"
     ]
    }
   ],
   "source": [
    "enc_tensor = sentence_sentence(korean_df[\"Q\"], tokenizer)\n",
    "dec_tensor = sentence_sentence(korean_df[\"A\"], tokenizer)\n",
    "\n",
    "print(\"encoder tensor 개수:\", f\"{len(enc_tensor):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-community",
   "metadata": {},
   "source": [
    "# 5. Transformer Model design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correct-confirmation",
   "metadata": {},
   "source": [
    "> 본 실습의 Chat-Bot 모델은 Transformer 모델로 제작한다.<br>\n",
    "> Transformer 모델은 Encoder-Decoder 구조로 이루어져 있으며, Encoder와 Decoder 각각 <code>positional_encoding</code>, <code>Multi-Head Attention</code>, <code>Position-wise Feed-Forward Network</code>를 결합한 형태로 구성되어 있다.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-annual",
   "metadata": {},
   "source": [
    "## 5.1 Transformer's Internal Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lonely-input",
   "metadata": {},
   "source": [
    "> Transformer 모델의 Encoder와 Decoder을 구성하는 <code>positional_encoding</code>, <code>Multi-Head Attention</code>, <code>Position-wise Feed-Forward Network</code>를 구현한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-lewis",
   "metadata": {},
   "source": [
    "### 5.1.1 Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-quarter",
   "metadata": {},
   "source": [
    "> 언어에서 문장 내 어순은, 언어를 이해하는 데 중요한 역할을 한다.<br>\n",
    "> 때문에, Layer에 입력하기 전에 어순 정보에 대한 데이터 전처리가 필요하다.<br>\n",
    "> <br>\n",
    "> 이에, Positional Encoding은 데이터가 Attention Layer에 들어가기 전에, 입력값인 단어 vector안에 단어의 위치 정보를 포함하는 기능을 수행하여 어순 정보를 제공하는 역할을 한다.<br>\n",
    ">\n",
    "> [참고 사이트 : positional encoding이란 무엇인가](https://skyjwoo.tistory.com/entry/positional-encoding%EC%9D%B4%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "minus-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i)/d_model)\n",
    "    \n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "    \n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "    \n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "    \n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-depth",
   "metadata": {},
   "source": [
    "### 5.1.2 Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-cooling",
   "metadata": {},
   "source": [
    "> Multi-Head Attention는 한 문장을 여러 head로 나누고, 이를 Self-Attention 하는 역할을 맡는다.<br>\n",
    "> <br>\n",
    "> 입력된 문장의 임베딩 벡터가 512차원이고, Head가 8개이면, 512/8 = 64개의 벡터를 하나씩 Scaled Dot Attention이 처리한다.<br>\n",
    "> 즉, 8명(8 heads)이 각각의 관점에서 가중치를 계산하고, 이후에 결과를 (+)합치는 작업을 진행한다.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "productive-tobacco",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "            \n",
    "        self.depth = d_model // self.num_heads\n",
    "            \n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "            \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "            \n",
    "\n",
    "    def split_heads(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        split_x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (batch_size, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "        \n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "            \n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask\n",
    "        )\n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "                \n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-russell",
   "metadata": {},
   "source": [
    "### 5.1.3 Position-wise Feed-Forward Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "native-wallpaper",
   "metadata": {},
   "source": [
    "> 이전 Multi-Head Attention에서 각 head가 자신의 관점으로만 문장을 Self-Attention 하면, 각 head 마다 Attention이 편향되는 문제가 발생한다.<br> \n",
    "> <br>\n",
    "> 이에, PoswiseFeedForwardNet은 각 head가 만들어낸 Self-Attention을 치우치지 않게 균등하게 조합하는 역할을 수행한다.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "studied-spell",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.w_1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.w_2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.w_1(x)\n",
    "        out = self.w_2(out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parliamentary-rouge",
   "metadata": {},
   "source": [
    "## 5.2 Encoder & Decoder Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-shame",
   "metadata": {},
   "source": [
    "> 5.1장에서 구현한 모듈을 활용하여 <code>Encoder</code>와 <code>Decoder</code>의 Layer와 Class를 구현한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fundamental-europe",
   "metadata": {},
   "source": [
    "### 5.2.1 Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "spiritual-mandate",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-domestic",
   "metadata": {},
   "source": [
    "### 5.2.2 Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "spiritual-neighborhood",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out, dec_enc_attn = self.enc_dec_attn(out, enc_out, enc_out, causality_mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "       \n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "printable-public",
   "metadata": {},
   "source": [
    "### 5.2.3 Encoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "southwest-while",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rural-heating",
   "metadata": {},
   "source": [
    "### 5.2.4 Decoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "chemical-christopher",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "                            \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "    \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secondary-exchange",
   "metadata": {},
   "source": [
    "### 5.2.5 Transformer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "latin-advertiser",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_layers, d_model, n_heads, d_ff,\n",
    "        src_vocab_size, tgt_vocab_size,\n",
    "        pos_len,\n",
    "        dropout=0.2,\n",
    "        shared=True\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared = shared\n",
    "\n",
    "        if shared: self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "            \n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "        \n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "        \n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
    "        \n",
    "        logits = self.fc(dec_out)\n",
    "        \n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-bleeding",
   "metadata": {},
   "source": [
    "## 5.3 Utility Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attended-melbourne",
   "metadata": {},
   "source": [
    "### 5.3.1 Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-control",
   "metadata": {},
   "source": [
    "> Masking은 Attention을 할 때에 PAD 토큰에 Attention을 주는 것을 방지해 주는 역할을 수행한다.<br>\n",
    "> Transformer의 Decoder에서도 PAD 토큰을 사용하지 않도록 <code>padding_mask</code>와 <code>causality_mask</code>를 결합하여 Masking을 구현한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "modified-liberia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "\n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "\n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-martin",
   "metadata": {},
   "source": [
    "### 5.3.2 LearningRateSchedule class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-prison",
   "metadata": {},
   "source": [
    "> Transformer의 학습률은 고정된 값을 사용하지 않고 학습 진행도에 따라 초기에는 높은 학습률로, 이후 점차 학습률을 감소시킨다.<br>\n",
    "> 이에, 모델이  유동적으로 변동하는 학습을 진행할 수 있도록 학습률과 옵티마이저를 설정하는 <code>LearningRateScheduler</code> 클래스를 구현한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "forced-distributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "realistic-jungle",
   "metadata": {},
   "source": [
    "# 6. Transformer Model train setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generic-battlefield",
   "metadata": {},
   "source": [
    "> 본 6장에서는 Chat-Bot에 사용할 Transformer Model을 학습하기 위한 사전 설정 과정을 기술한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passing-enlargement",
   "metadata": {},
   "source": [
    "## 6.1 Hyper parameter setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-palace",
   "metadata": {},
   "source": [
    "> Transformer Model에 사용할 <code>Loss</code> 함수를 재 정의하여  Masking 되지 않은 입력의 개수로 Scaling하는 과정을 추가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "according-semiconductor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-radio",
   "metadata": {},
   "source": [
    "> 모델의 <code>learing_rate</code>, <code>optimizer</code>, <code>loss rate</code>에 대한 객체를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "joined-vault",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = LearningRateScheduler(512)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "large-cricket",
   "metadata": {},
   "source": [
    "## 6.2 Train setp function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unsigned-breast",
   "metadata": {},
   "source": [
    "> 모델 학습시 각 스텝마다 수행할 학습 함수를 구현한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "varying-colombia",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    tgt_in = tgt[:, :-1]\n",
    "    gold = tgt[:, 1:]\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-watts",
   "metadata": {},
   "source": [
    "## 6.3 Transformer Model 번역 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effective-belief",
   "metadata": {},
   "source": [
    "> 모델의 성능을 시각적으로 확인하기 위한 번역 시각화 기능 함수를 구현한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "increasing-aaron",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, model, tokenizer, enc_tensor, dec_tensor):\n",
    "    enc_maxlen = enc_tensor.shape[-1]\n",
    "    dec_maxlen = dec_tensor.shape[-1]\n",
    "\n",
    "    sos_idx = tokenizer.word_index['<start>']\n",
    "    eos_idx = tokenizer.word_index['<end>']\n",
    "\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    m = Mecab()\n",
    "    sentence = m.morphs(sentence)\n",
    "\n",
    "    _input = tokenizer.texts_to_sequences([sentence])\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        _input,\n",
    "        maxlen=enc_maxlen,\n",
    "        padding='post'\n",
    "    )\n",
    "\n",
    "    ids = []\n",
    "    output = tf.expand_dims([sos_idx], 0)\n",
    "\n",
    "    for i in range(dec_maxlen):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = generate_masks(_input, output)\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = model(\n",
    "            _input, output, enc_padding_mask, combined_mask, dec_padding_mask)\n",
    "\n",
    "        predicted_id = tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "\n",
    "        if predicted_id == eos_idx:\n",
    "            result = tokenizer.sequences_to_texts([ids])\n",
    "            return result\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "    result = tokenizer.sequences_to_texts([ids])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similar-switch",
   "metadata": {},
   "source": [
    "> Transformer 인스턴스 객체를 생성하고, 모델 성능을 확인하기 위한 예문 리스트를 생성한다.<br>\n",
    "> Transformer 모델은 2개의 인코더와 디코더 클래스를 가지며, attention head는 8개, 드롭아웃 비율은 30%이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informed-range",
   "metadata": {},
   "source": [
    "## 6.4 Transformer model train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "geological-lender",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    n_layers=2,\n",
    "    d_model=256,\n",
    "    n_heads=8,\n",
    "    d_ff=256,\n",
    "    dropout=0.3,\n",
    "    pos_len=200,\n",
    "    shared=True,\n",
    "    src_vocab_size=6833, tgt_vocab_size=6833)\n",
    "\n",
    "examples = [\n",
    "    \"지루하다, 놀러가고 싶어.\",\n",
    "    \"오늘 일찍 일어났더니 피곤하다.\",\n",
    "    \"간만에 여자친구랑 데이트 하기로 했어.\",\n",
    "    \"집에 있는다는 소리야.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-cycle",
   "metadata": {},
   "source": [
    "> Transformer 모델을 훈련하는 함수를 구현한다. 10회 훈련마다 예문을 활용하여 모델의 성능을 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "square-grade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit(enc_tensor, dec_tensor, model, epochs, batch_size):\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        idx_list = list(range(0, enc_tensor.shape[0], batch_size))\n",
    "        random.shuffle(idx_list)\n",
    "        t = tqdm(idx_list)\n",
    "        \n",
    "        for (batch, idx) in enumerate(t):\n",
    "            batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "            train_step(\n",
    "                enc_tensor[idx:idx+batch_size],\n",
    "                dec_tensor[idx:idx+batch_size],\n",
    "                model, optimizer)\n",
    "            \n",
    "            total_loss += batch_loss\n",
    "            t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "            t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
    "            \n",
    "        for sentence in examples:\n",
    "            if(epoch+1) % 10 == 0:\n",
    "                result = translate(sentence, transformer, tokenizer, enc_tensor, dec_tensor)[0]\n",
    "                print('-'*30)\n",
    "                print('입력 문장 : %s' % (sentence))\n",
    "                print('모델 답변 : {}'.format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-threshold",
   "metadata": {},
   "source": [
    "> Transformer 모델을 64 batch size로 50회 반복 훈련한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "egyptian-sample",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdb2ffd053ff4f2087efe8256c8166b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe0ef100aa444e9da438ada86512141f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe94f6c7f137481ebd3367fd1e1e1339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80e364c45dab4e6c945f08c2681e7fcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7fa066a66fc46608291cbe6280f003d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cca2b24420914cacbca7f639d9a3d42c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5741a92a545d45cba896fb7e684bb1db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e24c5aaea9ea4d8480bd9e90c28512a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83b17a9022014461ab4492fcea1ea672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b88b718309ff4e82b27c226c7a8498e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "입력 문장 : 지루하다, 놀러가고 싶어.\n",
      "모델 답변 : 저 도 그럴 때 가 있 어요 .\n",
      "------------------------------\n",
      "입력 문장 : 오늘 일찍 일어났더니 피곤하다.\n",
      "모델 답변 : 푹 쉬 <UNK> .\n",
      "------------------------------\n",
      "입력 문장 : 간만에 여자친구랑 데이트 하기로 했어.\n",
      "모델 답변 : <UNK> 하 지 않 는다면 물 어 보 세요 .\n",
      "------------------------------\n",
      "입력 문장 : 집에 있는다는 소리야.\n",
      "모델 답변 : 좋 은 사람 이랑 좋 은데 아니 에요 .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a27048b5dd584f6097b19923acdc6732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e56ab0e18bf84f60b743ebcbd906bad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c5e4a3d6472471ea1f034f71c1c77b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3b446952522457393442c9f2548459a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4d5a324ca094fbe8bc9d0c4597f3158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06641ec6b6ab41988938d9b0e09a28c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be694ff59948459ea2cde9cf11e38efe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c467e20e6a04428a82e4273423f82aa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dda19b3f424d45729bca0404ca733c34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "887712609d344bf7b8d68237228b160c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "입력 문장 : 지루하다, 놀러가고 싶어.\n",
      "모델 답변 : 저 도 모르 겠 네요 .\n",
      "------------------------------\n",
      "입력 문장 : 오늘 일찍 일어났더니 피곤하다.\n",
      "모델 답변 : 푹 쉬 <UNK> .\n",
      "------------------------------\n",
      "입력 문장 : 간만에 여자친구랑 데이트 하기로 했어.\n",
      "모델 답변 : 가장 중요 한 <UNK> 거짓말 일 거 예요 .\n",
      "------------------------------\n",
      "입력 문장 : 집에 있는다는 소리야.\n",
      "모델 답변 : 모처럼 만 에 는 집 로 흘리 세요 .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f21c00ddfbc4851bbf850e3d2146aad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59bb5b448a6b41319ece6ea47b5982bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae3c3df240b445c9dd73a39f6f45a2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4da12486f3be4cb0900a67ae6b7addf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7da2c7cbf7a6458a8bd4fcd458338bbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b1fb9018d074df08e4b02037cb9c2e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a110a126c7874f77b0551b4e17fa83fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaf9a3224cdf47bbb54a86fba947807e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06a595a2e34c4011b590c6991cab9a24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12753b9e27ca469ba2c268b0a7b9d7bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "입력 문장 : 지루하다, 놀러가고 싶어.\n",
      "모델 답변 : 있 는 거 라고 생각 하 세요 .\n",
      "------------------------------\n",
      "입력 문장 : 오늘 일찍 일어났더니 피곤하다.\n",
      "모델 답변 : 금방 했 어요 .\n",
      "------------------------------\n",
      "입력 문장 : 간만에 여자친구랑 데이트 하기로 했어.\n",
      "모델 답변 : 너무 생길 때 하 지 마세요 .\n",
      "------------------------------\n",
      "입력 문장 : 집에 있는다는 소리야.\n",
      "모델 답변 : 쉽 지 기 도 하 죠 .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41956162aa4b4603ab056ce48ea30a6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccefb776d06c445d9c0b4cb1ceb2fd7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c1ef8eb5ebf4f4cb4e1195535ff0c30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "642e6583f34e4a26a1a6c501dad962b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aaf3873129842dca7c8606ea9f927e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6107391ad1d4abc92b0ef83715ccb7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3395da8af5dd4450827e3a26a955283f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ff2b847a962414cb400032faf830272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88341546f52343beb4050fddd4970b17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58d256ed3b2a4092bc489ffcd593dbfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "입력 문장 : 지루하다, 놀러가고 싶어.\n",
      "모델 답변 : 저 도 요 .\n",
      "------------------------------\n",
      "입력 문장 : 오늘 일찍 일어났더니 피곤하다.\n",
      "모델 답변 : 금방 지나갈 거 예요 .\n",
      "------------------------------\n",
      "입력 문장 : 간만에 여자친구랑 데이트 하기로 했어.\n",
      "모델 답변 : 좋 은 생각 만 해도 예쁘 네요 .\n",
      "------------------------------\n",
      "입력 문장 : 집에 있는다는 소리야.\n",
      "모델 답변 : 집 에 대한 최고 죠 .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3420a677c3394b87ba431ccbf6fa6dd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0517c7d46b744864870c43d1a3acaa1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4642f9b9694443f94e34d781ba7f8e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3753c6999ea4f5bb37715852c87bc44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "390d876067a24f05a68be3bf2a40164a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6dd5fb9edbd4433b4d08a9b55843f9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c42237184874b12ad234039d1148693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e56d63d0f032494295bb6f746131025d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c142b60f625484e92cba70f61311b53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a491e522cef489292560d08ef81a3dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "입력 문장 : 지루하다, 놀러가고 싶어.\n",
      "모델 답변 : 저 도 쉬 는 대로 해 보 세요 .\n",
      "------------------------------\n",
      "입력 문장 : 오늘 일찍 일어났더니 피곤하다.\n",
      "모델 답변 : 푹 쉬 세요 .\n",
      "------------------------------\n",
      "입력 문장 : 간만에 여자친구랑 데이트 하기로 했어.\n",
      "모델 답변 : 좋 은 선택 하 세요 .\n",
      "------------------------------\n",
      "입력 문장 : 집에 있는다는 소리야.\n",
      "모델 답변 : 그런 사람 안 어렵 겠 네요 .\n"
     ]
    }
   ],
   "source": [
    "model_fit(enc_tensor, dec_tensor, transformer, 50, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-schedule",
   "metadata": {},
   "source": [
    "> 추가로 동일한 하이퍼 파라미터 설정을 유지하여 10회 더 반복 훈련한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "municipal-scanning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e62e76dded54260ae4db53574fd5e14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3474d87db0c24f089339bd635164a53d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef2ac82c0ddb41598e7d5a2a3f15cf84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2478bb3f56de4674900535f901b6ca29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cdbc10c9b0a4fe1881322245a0ff13e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2070287541b443539f10052a6e406aef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e11b1076b841474ab65c7c90189f8f87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e480cec382e4492390156df6be750d23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "defb8f6ba83f410d894dc7a3fffa167b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f4ccacf3bce4983821bff076bfce58f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "입력 문장 : 지루하다, 놀러가고 싶어.\n",
      "모델 답변 : 저 랑 이야기 해요 .\n",
      "------------------------------\n",
      "입력 문장 : 오늘 일찍 일어났더니 피곤하다.\n",
      "모델 답변 : 푹 쉬 세요 .\n",
      "------------------------------\n",
      "입력 문장 : 간만에 여자친구랑 데이트 하기로 했어.\n",
      "모델 답변 : 좋 은 선택 하 셨 네요 .\n",
      "------------------------------\n",
      "입력 문장 : 집에 있는다는 소리야.\n",
      "모델 답변 : 내일 은 안 좋 아요 .\n"
     ]
    }
   ],
   "source": [
    "model_fit(enc_tensor, dec_tensor, transformer, 10, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-shooting",
   "metadata": {},
   "source": [
    "> 1번째 예시 문장<br>\n",
    "> <br>\n",
    "> 입력 문장 : 지루하다, 놀러가고 싶어.<br>\n",
    "> 모델 답변 : 저 랑 이야기 해요 .<br>\n",
    "> 평 가 - 지루해하는 질문자에게 대화를 권하며 심심함을 달래주려는 답변을 생성한 모습을 보여주었다.<br><br>\n",
    "\n",
    "> 2번째 예시 문장<br>\n",
    "> <br>\n",
    "> 입력 문장 : 오늘 일찍 일어났더니 피곤하다.<br>\n",
    "> 모델 답변 : 푹 쉬 세요 .<br>\n",
    "> 평 가 - 피곤하다는 의사를 표하는 질문자에게 휴식을 권하는 문장을 생성한 모습을 보여주었다.<br><br>\n",
    "\n",
    "\n",
    "> 3번째 예시 문장<br>\n",
    "> <br>\n",
    "> 입력 문장 : 간만에 여자친구랑 데이트 하기로 했어.<br>\n",
    "> 모델 답변 : 좋 은 선택 하 셨 네요 .<br>\n",
    "> 평 가 - 여가시간에 여자친구와 시간을 보내기로 한 질문자에게는 칭찬하는 문장을 생성한 모습을 보여주었다.<br><br>\n",
    "\n",
    "\n",
    "> 4번째 예시 문장<br>\n",
    "> <br>\n",
    "> 입력 문장 : 집에 있는다는 소리야.<br>\n",
    "> 모델 답변 : 내일 은 안 좋 아요 .<br>\n",
    "> 평 가 - 질문자의 의도가 무엇인지는 모르겠으나, 모델은 내일은 집에 있는것이 좋지 않다는 문장을 생성한 모습을 보여주었다.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpine-render",
   "metadata": {},
   "source": [
    "> 모델의 성능을 사람이 주관적으로 평가했을때는, 질문에 대해 준수한 문장 생성 성능을 보여주었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-trainer",
   "metadata": {},
   "source": [
    "# 7. model test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "second-catalog",
   "metadata": {},
   "source": [
    "> 문장 생성 모델의 성능을 측정하는 지표인 BLEU Score을 활용하여 훈련된 Transformer 모델의 성능을 확인하는 과정을 기술한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-briefs",
   "metadata": {},
   "source": [
    "## 7.1 BLEU Score Function"
   ]
  },
  {
   "attachments": {
    "%EC%BA%A1%EC%B2%98.PNG": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "monthly-comfort",
   "metadata": {},
   "source": [
    "> BLEU Score는 모든 N-gram에 대해서 점수를 측정하여 0~1 사이의 값을 가지며, 결과값에 100을 곱한 백분율 값으로 표기하여 성능을 평가한다.<br>\n",
    "> ![%EC%BA%A1%EC%B2%98.PNG](attachment:%EC%BA%A1%EC%B2%98.PNG)\n",
    "\n",
    "> BLEU Score는 50점을 넘으면 매우 좋은 성능이라 할 수 있으며, 보통 논문에서 제시하는 BLEU Score는 20점에서 높으면 40점의 성능을 보여준다.<br>\n",
    "> 본 성능 실험에서는 weights의 디폴트 값을 [0.25, 0.25, 0.25, 0.25]으로 설정하여 1-gram부터 4-gram까지의 점수에 가중치를 동일하게 부여한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "patent-issue",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu(reference, candidate, weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "    return sentence_bleu([reference], candidate, weights=weights, smoothing_function=SmoothingFunction().method1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-forestry",
   "metadata": {},
   "source": [
    "## 7.2 Beam Search Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-sending",
   "metadata": {},
   "source": [
    "> 기존 BLEU Score을 측정하는 <code>Greedy Decoding()</code> 대신 <code>Beam Search</code> 기법을 적용하여 모델 성능 측정 지표를 개선하고자 한다.<br>\n",
    ">  <br>\n",
    ">  <code>Beam Size</code>를 5로 설정한 <code>Beam Search Decoder</code>는 아래와 같은 과정을 진행한다.<br>\n",
    "> 1. 첫 단어로 5개의 단어를 생성하고 두 번째 단어로 각 첫 단어(5개 단어)에 대해 5순위까지 확률을 구하여 총 25개의 문장을 생성한다.<br>\n",
    "> 2. 25개의 문장들은 각 단어에 할당된 확률을 곱하여 구한 점수(존재 확률) 각각의 순위를 매긴다.<br>\n",
    "> 3. 점수 상위 5개의 표본만 살아남아 세 번째 단어를 구한다.<br>\n",
    "> 위 과정을 N-gram의 매 분기마다 반복하면 최종적으로 점수가 가장 높은 5개의 문장을 얻게 된다.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "blessed-starter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 단어의 확률값 계산 함수\n",
    "def calc_prob(src_ids, tgt_ids, model):\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = generate_masks(\n",
    "        src_ids, tgt_ids\n",
    "    )\n",
    "    \n",
    "    predictions, enc_attns, dec_attns, dec_enc_attns = model(\n",
    "        src_ids, tgt_ids, enc_padding_mask, combined_mask, dec_padding_mask\n",
    "    )\n",
    "    return tf.math.softmax(predictions, axis=-1)\n",
    "\n",
    "\n",
    "# Beam Search를 기반으로 동작하는 decoder 함수\n",
    "def beam_search_decoder(\n",
    "    sentence, model, tokenizer,\n",
    "    enc_maxlen, dec_maxlen,\n",
    "    beam_size\n",
    "):\n",
    "    sos_idx = tokenizer.word_index['<start>']\n",
    "    eos_idx = tokenizer.word_index['<end>']\n",
    "\n",
    "    tokens = tokenizer.texts_to_sequences([sentence])\n",
    "    src_in = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokens,\n",
    "        maxlen=enc_maxlen,\n",
    "        padding='post'\n",
    "    )\n",
    "\n",
    "    pred_cache = np.zeros((beam_size * beam_size, dec_maxlen), dtype=np.long)\n",
    "    pred = np.zeros((beam_size, dec_maxlen), dtype=np.long)\n",
    "\n",
    "    eos_flag = np.zeros((beam_size, ), dtype=np.long)\n",
    "    scores = np.ones((beam_size, ))\n",
    "\n",
    "    pred[:, 0] = sos_idx\n",
    "\n",
    "    dec_in = tf.expand_dims(pred[0, :1], 0)\n",
    "    prob = calc_prob(src_in, dec_in, model)[0, -1].numpy()\n",
    "    \n",
    "    \n",
    "    for seq_pos in range(1, dec_maxlen):\n",
    "        score_cache = np.ones((beam_size * beam_size, ))\n",
    "\n",
    "        # init\n",
    "        for branch_idx in range(beam_size):\n",
    "            cache_pos = branch_idx*beam_size\n",
    "\n",
    "            score_cache[cache_pos:cache_pos+beam_size] = scores[branch_idx]\n",
    "            pred_cache[cache_pos:cache_pos+beam_size, :seq_pos] = \\\n",
    "            pred[branch_idx, :seq_pos]\n",
    "\n",
    "        for branch_idx in range(beam_size):\n",
    "            cache_pos = branch_idx*beam_size\n",
    "\n",
    "            if seq_pos != 1:   # 모든 Branch를 로 시작하는 경우를 방지\n",
    "                dec_in = pred_cache[branch_idx, :seq_pos]\n",
    "                dec_in = tf.expand_dims(dec_in, 0)\n",
    "\n",
    "                prob = calc_prob(src_in, dec_in, model)[0, -1].numpy()\n",
    "\n",
    "            for beam_idx in range(beam_size):\n",
    "                max_idx = np.argmax(prob)\n",
    "\n",
    "                score_cache[cache_pos+beam_idx] *= prob[max_idx]\n",
    "                pred_cache[cache_pos+beam_idx, seq_pos] = max_idx\n",
    "\n",
    "                prob[max_idx] = -1\n",
    "\n",
    "        for beam_idx in range(beam_size):\n",
    "            if eos_flag[beam_idx] == -1: continue\n",
    "\n",
    "            max_idx = np.argmax(score_cache)\n",
    "            prediction = pred_cache[max_idx, :seq_pos+1]\n",
    "\n",
    "            pred[beam_idx, :seq_pos+1] = prediction\n",
    "            scores[beam_idx] = score_cache[max_idx]\n",
    "            score_cache[max_idx] = -1\n",
    "\n",
    "            if prediction[-1] == eos_idx:\n",
    "                eos_flag[beam_idx] = -1\n",
    "    return pred\n",
    "\n",
    "\n",
    "# 생성된 문장에 대한 BLEU Score 출력 함수\n",
    "def beam_bleu(reference, ids, tokenizer, verbose=False):\n",
    "    reference = reference.split()\n",
    "\n",
    "    total_score = 0.0\n",
    "    for _id in ids:\n",
    "        seq2text = tokenizer.sequences_to_texts([_id])[0]\n",
    "        _idx =  seq2text.find(\"<end>\")\n",
    "        seq2text = seq2text[6:_idx]\n",
    "        candidate = seq2text.split()\n",
    "        score = calculate_bleu(reference, candidate)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Reference:\".ljust(10), \" \".join(reference))\n",
    "            print(\"Candidate:\".ljust(10), \" \".join(candidate), end=\"\\n\")\n",
    "            print(\"BLEU:\".ljust(10), f\"{calculate_bleu(reference, candidate):.3f}\")\n",
    "        \n",
    "        total_score += score\n",
    "        \n",
    "    return total_score / len(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placed-imaging",
   "metadata": {},
   "source": [
    "## 7.3 Transformer Model Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-offering",
   "metadata": {},
   "source": [
    "> 3.5장에서 생성한 100개의 테스트용 문장을 활용하여 훈련된 Transformer model의 성능을 Beam Search Decoder으로 평가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "public-cartoon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " BLEU score: 0.22\n"
     ]
    }
   ],
   "source": [
    "aver_bleu = 0\n",
    "for _, que, ans in test_data.itertuples():\n",
    "    ids = beam_search_decoder(\n",
    "        que,\n",
    "        transformer, tokenizer,\n",
    "        enc_tensor.shape[-1], dec_tensor.shape[-1],\n",
    "        beam_size=5\n",
    "    )\n",
    "    \n",
    "    test_dec_sentence = tokenizer.sequences_to_texts([que])[0]\n",
    "    _idx = test_dec_sentence.find(\"<end>\")\n",
    "    test_dec_sentence = test_dec_sentence[6:_idx]\n",
    "    \n",
    "    aver_bleu += beam_bleu(test_dec_sentence, ids, tokenizer, verbose=False)\n",
    "    \n",
    "\n",
    "print(f\" BLEU score: {aver_bleu:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-preference",
   "metadata": {},
   "source": [
    "> 평가 결과 0.22로, 22의 점수를 기록했으며, 요점은 명확하지만 많은 문법적 오류가 있는 모델이라는 평가를 받았다.<br>\n",
    "> 이는 보통 논문에서 제시하는 모델의 성능이므로 본 연구에서 훈련된 Transformer model의 성능은 일반적인 성능을 보여준다고 할 수 있다.<br>\n",
    "\n",
    "> 점수별 상세 내용은 아래 링크를 참고하여 확인할 수 있다.<br>\n",
    "> [BLEU Score의 점수대별 해석표](https://cloud.google.com/translate/automl/docs/evaluate?hl=ko#bleu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
